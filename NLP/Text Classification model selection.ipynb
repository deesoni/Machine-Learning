{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Misc/stack-overflow-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[pd.notnull(df['tags'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is causing this behavior  in our c# datet...</td>\n",
       "      <td>c#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>have dynamic html load as if it was in an ifra...</td>\n",
       "      <td>asp.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how to convert a float value in to min:sec  i ...</td>\n",
       "      <td>objective-c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.net framework 4 redistributable  just wonderi...</td>\n",
       "      <td>.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trying to calculate and print the mean and its...</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post         tags\n",
       "0  what is causing this behavior  in our c# datet...           c#\n",
       "1  have dynamic html load as if it was in an ifra...      asp.net\n",
       "2  how to convert a float value in to min:sec  i ...  objective-c\n",
       "3  .net framework 4 redistributable  just wonderi...         .net\n",
       "4  trying to calculate and print the mean and its...       python"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10286120"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['post'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.net', 'angularjs', 'asp.net', 'html', 'javascript', 'c#', 'jquery', 'objective-c', 'sql', 'ios', 'c++', 'java', 'python', 'mysql', 'ruby-on-rails', 'css', 'php', 'iphone', 'c', 'android']\n"
     ]
    }
   ],
   "source": [
    "my_tags = (list(df['tags'].value_counts().index))\n",
    "print(my_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2618120d518>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmAAAAExCAYAAADbUR4fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzARMkiRpYCZgkiRJAzMBkyRJGtj/D3NU6FpKd6LOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x261818c4be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "df.tags.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_plot(index):\n",
    "    example = df[df.index == index].values[0]\n",
    "    if len(example) > 0:\n",
    "        print(example[0], '\\n')\n",
    "        print('Tag:', example[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when we need interface c# <blockquote>    <strong>possible duplicate:</strong><br>   <a href= https://stackoverflow.com/questions/240152/why-would-i-want-to-use-interfaces >why would i want to use interfaces </a>   <a href= https://stackoverflow.com/questions/9451868/why-i-need-interface >why i need interface </a>    </blockquote>     i want to know where and when to use it     for example    <pre><code>interface idemo {  // function prototype  public void show(); }  // first class using the interface class myclass1 : idemo {  public void show()  {   // function body comes here   response.write( i m in myclass );  }  }  // second class using the interface class myclass2 : idemo {  public void show()   {   // function body comes here   response.write( i m in myclass2 );   response.write( so  what  );  } </code></pre>   these two classes has the same function name with different body. this can be even achieved without interface. then why we need an interface where and when to use it \n",
      "\n",
      "Tag: c#\n"
     ]
    }
   ],
   "source": [
    "print_plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to chain expressions inside ngclass when using the {...}[] form  how can i add another expression to an <code>ng-class</code> directive that uses this form:   <pre><code>ng-class= {true: loading   false: loading-done }[data.loader===null]  </code></pre>   i d like to add something like this to the list:   <pre><code>{highlight:isspecial} </code></pre>   is it possible without expanding the first expression     thanks. \n",
      "\n",
      "Tag: angularjs\n"
     ]
    }
   ],
   "source": [
    "print_plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['post'] = df['post'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need interface c# possible duplicate would want use interfaces need interface want know use example interface idemo function prototype public void show first class using interface class myclass1 idemo public void show function body comes response write myclass second class using interface class myclass2 idemo public void show function body comes response write myclass2 response write two classes function name different body even achieved without interface need interface use \n",
      "\n",
      "Tag: c#\n"
     ]
    }
   ],
   "source": [
    "print_plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3880052"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['post'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.post\n",
    "y = df.tags\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Imp*\n",
    "\n",
    "The next steps includes feature engineering. We will convert our text documents to a matrix of token counts (CountVectorizer), then transform a count matrix to a normalized tf-idf representation (tf-idf transformer). After that, we train several classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB())\n",
    "              ])\n",
    "\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score 0.7399166666666667\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         .net       0.65      0.64      0.64       613\n",
      "    angularjs       0.94      0.88      0.91       620\n",
      "      asp.net       0.85      0.94      0.89       587\n",
      "         html       0.70      0.81      0.75       586\n",
      "   javascript       0.75      0.87      0.81       599\n",
      "           c#       0.70      0.54      0.61       589\n",
      "       jquery       0.80      0.77      0.79       594\n",
      "  objective-c       0.67      0.89      0.76       610\n",
      "          sql       0.60      0.57      0.58       617\n",
      "          ios       0.59      0.65      0.62       587\n",
      "          c++       0.72      0.50      0.59       611\n",
      "         java       0.82      0.78      0.80       594\n",
      "       python       0.81      0.59      0.68       619\n",
      "        mysql       0.66      0.82      0.73       574\n",
      "ruby-on-rails       0.61      0.82      0.70       584\n",
      "          css       0.69      0.65      0.67       578\n",
      "          php       0.80      0.76      0.78       591\n",
      "       iphone       0.90      0.83      0.86       608\n",
      "            c       0.94      0.88      0.91       638\n",
      "      android       0.74      0.63      0.68       601\n",
      "\n",
      "    micro avg       0.74      0.74      0.74     12000\n",
      "    macro avg       0.75      0.74      0.74     12000\n",
      " weighted avg       0.75      0.74      0.74     12000\n",
      "\n",
      "Wall time: 2.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy score {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print(classification_report(y_test, y_pred, target_names=my_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...dom_state=42, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(alpha=1e-3, random_state=42, max_iter=5))])\n",
    "\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7911666666666667\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         .net       0.77      0.66      0.71       613\n",
      "    angularjs       0.87      0.93      0.90       620\n",
      "      asp.net       0.88      0.98      0.93       587\n",
      "         html       0.80      0.84      0.82       586\n",
      "   javascript       0.75      0.89      0.82       599\n",
      "           c#       0.76      0.41      0.54       589\n",
      "       jquery       0.86      0.73      0.79       594\n",
      "  objective-c       0.68      0.95      0.79       610\n",
      "          sql       0.82      0.50      0.62       617\n",
      "          ios       0.71      0.60      0.65       587\n",
      "          c++       0.72      0.64      0.68       611\n",
      "         java       0.81      0.88      0.84       594\n",
      "       python       0.78      0.78      0.78       619\n",
      "        mysql       0.84      0.85      0.85       574\n",
      "ruby-on-rails       0.82      0.80      0.81       584\n",
      "          css       0.70      0.68      0.69       578\n",
      "          php       0.78      0.91      0.84       591\n",
      "       iphone       0.84      0.95      0.89       608\n",
      "            c       0.87      0.95      0.91       638\n",
      "      android       0.77      0.89      0.82       601\n",
      "\n",
      "    micro avg       0.79      0.79      0.79     12000\n",
      "    macro avg       0.79      0.79      0.78     12000\n",
      " weighted avg       0.79      0.79      0.78     12000\n",
      "\n",
      "Wall time: 2.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', LogisticRegression(C=1e5, n_jobs=1))])\n",
    "\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score 0.7769166666666667\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         .net       0.67      0.62      0.65       613\n",
      "    angularjs       0.93      0.91      0.92       620\n",
      "      asp.net       0.96      0.95      0.95       587\n",
      "         html       0.77      0.77      0.77       586\n",
      "   javascript       0.77      0.82      0.80       599\n",
      "           c#       0.60      0.59      0.59       589\n",
      "       jquery       0.80      0.76      0.78       594\n",
      "  objective-c       0.79      0.85      0.82       610\n",
      "          sql       0.67      0.67      0.67       617\n",
      "          ios       0.61      0.58      0.59       587\n",
      "          c++       0.63      0.63      0.63       611\n",
      "         java       0.83      0.82      0.83       594\n",
      "       python       0.76      0.76      0.76       619\n",
      "        mysql       0.84      0.84      0.84       574\n",
      "ruby-on-rails       0.79      0.82      0.81       584\n",
      "          css       0.62      0.64      0.63       578\n",
      "          php       0.82      0.82      0.82       591\n",
      "       iphone       0.90      0.92      0.91       608\n",
      "            c       0.97      0.94      0.95       638\n",
      "      android       0.78      0.82      0.80       601\n",
      "\n",
      "    micro avg       0.78      0.78      0.78     12000\n",
      "    macro avg       0.78      0.78      0.78     12000\n",
      " weighted avg       0.78      0.78      0.78     12000\n",
      "\n",
      "Wall time: 2.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('accuracy score {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print(classification_report(y_test, y_pred, target_names=my_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2vec embedding and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"../Misc/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Memorial_Hospital',\n",
       " 'Seniors',\n",
       " 'memorandum',\n",
       " 'elephant',\n",
       " 'Trump',\n",
       " 'Census',\n",
       " 'pilgrims',\n",
       " 'De',\n",
       " 'Dogs',\n",
       " '###-####_ext',\n",
       " 'chaotic',\n",
       " 'forgive',\n",
       " 'scholar',\n",
       " 'Lottery',\n",
       " 'decreasing',\n",
       " 'Supervisor',\n",
       " 'fundamentally',\n",
       " 'Fitness',\n",
       " 'abundance',\n",
       " 'Hold']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "list(islice(wv.vocab, 13030, 13050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.3, random_state = 42)\n",
    "\n",
    "test_tokenized = test.apply(lambda x: w2v_tokenize_text(x['post']), axis=1).values\n",
    "train_tokenized = train.apply(lambda x: w2v_tokenize_text(x['post']), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:cannot compute similarity with no input []\n"
     ]
    }
   ],
   "source": [
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg = logreg.fit(X_train_word_average, train['tags'])\n",
    "y_pred = logreg.predict(X_test_word_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6704166666666667\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         .net       0.65      0.59      0.62       613\n",
      "    angularjs       0.78      0.81      0.79       620\n",
      "      asp.net       0.79      0.82      0.80       587\n",
      "         html       0.69      0.71      0.70       586\n",
      "   javascript       0.72      0.78      0.75       599\n",
      "           c#       0.46      0.42      0.44       589\n",
      "       jquery       0.68      0.65      0.67       594\n",
      "  objective-c       0.73      0.82      0.77       610\n",
      "          sql       0.61      0.59      0.60       617\n",
      "          ios       0.56      0.56      0.56       587\n",
      "          c++       0.56      0.53      0.55       611\n",
      "         java       0.66      0.66      0.66       594\n",
      "       python       0.64      0.65      0.65       619\n",
      "        mysql       0.60      0.59      0.59       574\n",
      "ruby-on-rails       0.70      0.71      0.71       584\n",
      "          css       0.50      0.49      0.49       578\n",
      "          php       0.72      0.74      0.73       591\n",
      "       iphone       0.77      0.77      0.77       608\n",
      "            c       0.83      0.83      0.83       638\n",
      "      android       0.67      0.69      0.68       601\n",
      "\n",
      "    micro avg       0.67      0.67      0.67     12000\n",
      "    macro avg       0.67      0.67      0.67     12000\n",
      " weighted avg       0.67      0.67      0.67     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, test.tags))\n",
    "print(classification_report(test.tags, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2vec and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc='progress-bar')\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentences(corpus, label_type):\n",
    "    \n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v.split(), [label]))\n",
    "    \n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.post, df.tags, random_state=0, test_size=0.3)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['r', 'cannot', 'resolved', 'variable', 'importing', 'project', 'pasting', 'problems', 'details', 'r', 'cannot', 'resolved', 'variable', 'common', 'problem', 'checked', 'res', 'folder', 'done', 'refreshing', 'project', 'cleaning', 'project', 'validate', 'still', 'error', 'resolved', 'help', 'guys', '2012', '12', '14', '02', '12', '38', 'com', 'example', 'android', 'livecubes', 'cube2', 'cubewallpaper2settings', 'res', 'drawable', 'ic_launcher_wallpaper', 'png', '0', 'error', 'resource', 'entry', 'ic_launcher_wallpaper', 'already', 'defined', '2012', '12', '14', '02', '12', '38', 'com', 'example', 'android', 'livecubes', 'cube2', 'cubewallpaper2settings', 'res', 'drawable', 'ic_launcher_wallpaper', 'html', '0', 'originally', 'defined', '2012', '12', '14', '02', '12', '38', 'com', 'example', 'android', 'livecubes', 'cube2', 'cubewallpaper2settings', 'res', 'xml', 'cube1', 'xml', '0', 'error', 'resource', 'entry', 'cube1', 'already', 'defined', '2012', '12', '14', '02', '12', '38', 'com', 'example', 'android', 'livecubes', 'cube2', 'cubewallpaper2settings', 'res', 'xml', 'cube1', 'html', '0', 'originally', 'defined', '2012', '12', '14', '02', '12', '38', 'com', 'example', 'android', 'livecubes', 'cube2', 'cubewallpaper2settings', 'res', 'xml', 'cube2', 'xml', '0', 'error', 'resource', 'entry', 'cube2', 'already', 'defined', '2012', '12', '14', '02', '12', '38', 'com', 'example', 'android', 'livecubes', 'cube2', 'cubewallpaper2settings', 'res', 'xml', 'cube2', 'html', '0', 'originally', 'defined', '2012', '12', '14', '02', '12', '38', 'com', 'example', 'android', 'livecubes', 'cube2', 'cubewallpaper2settings', 'res', 'xml', 'cube2_settings', 'xml', '0', 'error', 'resource', 'entry', 'cube2_settings', 'already', 'defined', '2012', '12', '14', '02', '12', '38', 'com', 'example', 'android', 'livecubes', 'cube2', 'cubewallpaper2settings', 'res', 'xml', 'cube2_settings', 'html', '0', 'originally', 'defined', '2012', '12', '14', '02', '12', '38', 'com', 'example', 'android', 'livecubes', 'cube2', 'cubewallpaper2settings', 'f', 'sample', 'projects', 'cube', 'res', 'values', 'index', 'html', '112', 'error', 'error', 'parsing', 'xml', 'mismatched', 'tag', '2012', '12', '14', '02', '12', '38', 'com', 'example', 'android', 'livecubes', 'cube2', 'cubewallpaper2settings', 'f', 'sample', 'projects', 'cube', 'res', 'values', 'shapes', 'html', '112', 'error', 'error', 'parsing', 'xml', 'mismatched', 'tag', '2012', '12', '14', '02', '12', '38', 'com', 'example', 'android', 'livecubes', 'cube2', 'cubewallpaper2settings', 'f', 'sample', 'projects', 'cube', 'res', 'values', 'strings', 'html', '112', 'error', 'error', 'parsing', 'xml', 'mismatched', 'tag', '2012', '12', '14', '02', '12', '38', 'com', 'example', 'android', 'livecubes', 'cube2', 'cubewallpaper2settings', 'f', 'sample', 'projects', 'cube', 'res', 'xml', 'cube1', 'html', '112', 'error', 'error', 'parsing', 'xml', 'mismatched', 'tag', '2012', '12', '14', '02', '12', '38', 'com', 'example', 'android', 'livecubes', 'cube2', 'cubewallpaper2settings', 'f', 'sample', 'projects', 'cube', 'res', 'xml', 'cube2', 'html', '112', 'error', 'error', 'parsing', 'xml', 'mismatched', 'tag', '2012', '12', '14', '02', '12', '38', 'com', 'example', 'android', 'livecubes', 'cube2', 'cubewallpaper2settings', 'f', 'sample', 'projects', 'cube', 'res', 'xml', 'cube2_settings', 'html', '112', 'error', 'error', 'parsing', 'xml', 'mismatched', 'tag', '2012', '12', '14', '02', '12', '38', 'com', 'example', 'android', 'livecubes', 'cube2', 'cubewallpaper2settings', 'f', 'sample', 'projects', 'cube', 'res', 'xml', 'index', 'html', '112', 'error', 'error', 'parsing', 'xml', 'mismatched', 'tag', 'copyright', 'c', '2009', 'google', 'inc', 'licensed', 'apache', 'license', 'version', '2', '0', 'license', 'may', 'use', 'file', 'except', 'compliance', 'license', 'may', 'obtain', 'copy', 'license', 'http', 'www', 'apache', 'org', 'licenses', 'license', '2', '0', 'unless', 'required', 'applicable', 'law', 'agreed', 'writing', 'software', 'distributed', 'license', 'distributed', 'basis', 'without', 'warranties', 'conditions', 'kind', 'either', 'express', 'implied', 'see', 'license', 'specific', 'language', 'governing', 'permissions', 'limitations', 'license', 'package', 'com', 'example', 'android', 'livecubes', 'cube2', 'import', 'com', 'example', 'android', 'livecubes', 'cube2', 'import', 'android', 'content', 'sharedpreferences', 'import', 'android', 'os', 'bundle', 'import', 'android', 'preference', 'preferenceactivity', 'public', 'class', 'cubewallpaper2settings', 'extends', 'preferenceactivity', 'implements', 'sharedpreferences', 'onsharedpreferencechangelistener', 'override', 'protected', 'void', 'oncreate', 'bundle', 'icicle', 'super', 'oncreate', 'icicle', 'getpreferencemanager', 'setsharedpreferencesname', 'cubewallpaper2', 'shared_prefs_name', 'addpreferencesfromresource', 'r', 'xml', 'cube2_settings', 'getpreferencemanager', 'getsharedpreferences', 'registeronsharedpreferencechangelistener', 'override', 'protected', 'void', 'onresume', 'super', 'onresume', 'override', 'protected', 'void', 'ondestroy', 'getpreferencemanager', 'getsharedpreferences', 'unregisteronsharedpreferencechangelistener', 'super', 'ondestroy', 'public', 'void', 'onsharedpreferencechanged', 'sharedpreferences', 'sharedpreferences', 'string', 'key'], tags=['Train_2'])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 1111508.21it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 1250370.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 1161125.33it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 994665.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 926882.16it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 909437.71it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 1000340.82it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 1134822.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 967706.02it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 1250323.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 1000066.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 1000334.85it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 995739.57it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 852409.85it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 1667168.42it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 357247.84it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 1250332.83it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 947726.10it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 1044905.77it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 1303115.10it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 1280087.90it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 1105058.29it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 977089.68it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 1023381.62it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 987406.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 1330965.23it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 976344.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 1239341.67it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 1086389.13it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 1209936.10it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:00<00:00, 1249392.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(train_vectors_dbow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logreg = logreg.fit(train_vectors_dbow, y_train)\n",
    "y_pred = logreg.predict(test_vectors_dbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8105\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         .net       0.71      0.68      0.69       589\n",
      "    angularjs       0.91      0.92      0.91       661\n",
      "      asp.net       0.92      0.93      0.92       606\n",
      "         html       0.80      0.82      0.81       613\n",
      "   javascript       0.86      0.89      0.88       601\n",
      "           c#       0.72      0.72      0.72       585\n",
      "       jquery       0.87      0.85      0.86       621\n",
      "  objective-c       0.79      0.83      0.81       587\n",
      "          sql       0.68      0.65      0.67       560\n",
      "          ios       0.69      0.65      0.67       611\n",
      "          c++       0.66      0.67      0.67       593\n",
      "         java       0.83      0.86      0.84       581\n",
      "       python       0.80      0.76      0.78       608\n",
      "        mysql       0.85      0.86      0.85       593\n",
      "ruby-on-rails       0.84      0.83      0.83       592\n",
      "          css       0.72      0.68      0.70       597\n",
      "          php       0.86      0.86      0.86       604\n",
      "       iphone       0.91      0.94      0.92       610\n",
      "            c       0.94      0.96      0.95       595\n",
      "      android       0.81      0.83      0.82       593\n",
      "\n",
      "    micro avg       0.81      0.81      0.81     12000\n",
      "    macro avg       0.81      0.81      0.81     12000\n",
      " weighted avg       0.81      0.81      0.81     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 28000\n",
      "Test size: 12000\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(df) * .7)\n",
    "print (\"Train size: %d\" % train_size)\n",
    "print (\"Test size: %d\" % (len(df) - train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_posts = df['post'][:train_size]\n",
    "train_tags = df['tags'][:train_size]\n",
    "\n",
    "test_posts = df['post'][train_size:]\n",
    "test_tags = df['tags'][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenize.fit_on_texts(train_posts)\n",
    "X_train = tokenize.texts_to_matrix(train_posts)\n",
    "X_test = tokenize.texts_to_matrix(test_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (28000, 1000)\n",
      "x_test shape: (12000, 1000)\n",
      "y_train shape: (28000, 20)\n",
      "y_test shape: (12000, 20)\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', X_train.shape)\n",
    "print('x_test shape:', X_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25200 samples, validate on 2800 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17152/25200 [===================>..........] - ETA: 30:03 - loss: 2.9963 - acc: 0.03 - ETA: 15:27 - loss: 3.0177 - acc: 0.03 - ETA: 7:54 - loss: 3.0494 - acc: 0.0312 - ETA: 5:22 - loss: 3.0188 - acc: 0.046 - ETA: 4:06 - loss: 3.0084 - acc: 0.062 - ETA: 3:03 - loss: 2.9727 - acc: 0.079 - ETA: 2:27 - loss: 2.9456 - acc: 0.087 - ETA: 2:03 - loss: 2.9300 - acc: 0.104 - ETA: 1:47 - loss: 2.9168 - acc: 0.118 - ETA: 1:38 - loss: 2.9003 - acc: 0.134 - ETA: 1:28 - loss: 2.8745 - acc: 0.156 - ETA: 1:20 - loss: 2.8474 - acc: 0.176 - ETA: 1:14 - loss: 2.8262 - acc: 0.186 - ETA: 1:08 - loss: 2.7934 - acc: 0.211 - ETA: 1:06 - loss: 2.7843 - acc: 0.216 - ETA: 1:03 - loss: 2.7643 - acc: 0.229 - ETA: 59s - loss: 2.7336 - acc: 0.248 - ETA: 57s - loss: 2.7145 - acc: 0.26 - ETA: 54s - loss: 2.6952 - acc: 0.26 - ETA: 52s - loss: 2.6656 - acc: 0.28 - ETA: 50s - loss: 2.6350 - acc: 0.29 - ETA: 48s - loss: 2.6085 - acc: 0.31 - ETA: 46s - loss: 2.5804 - acc: 0.32 - ETA: 44s - loss: 2.5528 - acc: 0.33 - ETA: 43s - loss: 2.5256 - acc: 0.34 - ETA: 41s - loss: 2.4932 - acc: 0.35 - ETA: 40s - loss: 2.4636 - acc: 0.37 - ETA: 39s - loss: 2.4418 - acc: 0.37 - ETA: 38s - loss: 2.4181 - acc: 0.39 - ETA: 37s - loss: 2.3906 - acc: 0.39 - ETA: 36s - loss: 2.3712 - acc: 0.40 - ETA: 36s - loss: 2.3516 - acc: 0.41 - ETA: 35s - loss: 2.3396 - acc: 0.41 - ETA: 35s - loss: 2.3225 - acc: 0.41 - ETA: 34s - loss: 2.2956 - acc: 0.42 - ETA: 33s - loss: 2.2731 - acc: 0.43 - ETA: 33s - loss: 2.2576 - acc: 0.43 - ETA: 32s - loss: 2.2349 - acc: 0.44 - ETA: 31s - loss: 2.2131 - acc: 0.45 - ETA: 31s - loss: 2.1892 - acc: 0.45 - ETA: 30s - loss: 2.1719 - acc: 0.46 - ETA: 30s - loss: 2.1577 - acc: 0.46 - ETA: 29s - loss: 2.1313 - acc: 0.47 - ETA: 29s - loss: 2.1146 - acc: 0.48 - ETA: 28s - loss: 2.0995 - acc: 0.48 - ETA: 28s - loss: 2.0826 - acc: 0.49 - ETA: 28s - loss: 2.0654 - acc: 0.49 - ETA: 27s - loss: 2.0438 - acc: 0.50 - ETA: 27s - loss: 2.0341 - acc: 0.50 - ETA: 27s - loss: 2.0133 - acc: 0.50 - ETA: 26s - loss: 1.9916 - acc: 0.51 - ETA: 26s - loss: 1.9727 - acc: 0.51 - ETA: 26s - loss: 1.9632 - acc: 0.52 - ETA: 25s - loss: 1.9456 - acc: 0.52 - ETA: 25s - loss: 1.9275 - acc: 0.52 - ETA: 25s - loss: 1.9190 - acc: 0.53 - ETA: 24s - loss: 1.9013 - acc: 0.53 - ETA: 24s - loss: 1.8851 - acc: 0.53 - ETA: 24s - loss: 1.8672 - acc: 0.54 - ETA: 23s - loss: 1.8572 - acc: 0.54 - ETA: 23s - loss: 1.8485 - acc: 0.54 - ETA: 23s - loss: 1.8350 - acc: 0.54 - ETA: 23s - loss: 1.8193 - acc: 0.55 - ETA: 22s - loss: 1.8045 - acc: 0.55 - ETA: 22s - loss: 1.7891 - acc: 0.55 - ETA: 22s - loss: 1.7772 - acc: 0.56 - ETA: 22s - loss: 1.7695 - acc: 0.56 - ETA: 22s - loss: 1.7545 - acc: 0.56 - ETA: 21s - loss: 1.7408 - acc: 0.57 - ETA: 21s - loss: 1.7283 - acc: 0.57 - ETA: 21s - loss: 1.7182 - acc: 0.57 - ETA: 21s - loss: 1.7076 - acc: 0.57 - ETA: 20s - loss: 1.6968 - acc: 0.57 - ETA: 20s - loss: 1.6904 - acc: 0.58 - ETA: 20s - loss: 1.6792 - acc: 0.58 - ETA: 20s - loss: 1.6690 - acc: 0.58 - ETA: 20s - loss: 1.6580 - acc: 0.58 - ETA: 19s - loss: 1.6465 - acc: 0.59 - ETA: 19s - loss: 1.6350 - acc: 0.59 - ETA: 19s - loss: 1.6246 - acc: 0.59 - ETA: 19s - loss: 1.6134 - acc: 0.59 - ETA: 19s - loss: 1.6038 - acc: 0.59 - ETA: 18s - loss: 1.5955 - acc: 0.60 - ETA: 18s - loss: 1.5848 - acc: 0.60 - ETA: 18s - loss: 1.5766 - acc: 0.60 - ETA: 18s - loss: 1.5702 - acc: 0.60 - ETA: 18s - loss: 1.5609 - acc: 0.60 - ETA: 18s - loss: 1.5497 - acc: 0.61 - ETA: 17s - loss: 1.5416 - acc: 0.61 - ETA: 17s - loss: 1.5337 - acc: 0.61 - ETA: 17s - loss: 1.5280 - acc: 0.61 - ETA: 17s - loss: 1.5186 - acc: 0.61 - ETA: 17s - loss: 1.5113 - acc: 0.62 - ETA: 17s - loss: 1.5030 - acc: 0.62 - ETA: 17s - loss: 1.4968 - acc: 0.62 - ETA: 16s - loss: 1.4877 - acc: 0.62 - ETA: 16s - loss: 1.4807 - acc: 0.62 - ETA: 16s - loss: 1.4727 - acc: 0.63 - ETA: 16s - loss: 1.4685 - acc: 0.63 - ETA: 16s - loss: 1.4607 - acc: 0.63 - ETA: 16s - loss: 1.4575 - acc: 0.63 - ETA: 16s - loss: 1.4502 - acc: 0.63 - ETA: 15s - loss: 1.4434 - acc: 0.63 - ETA: 15s - loss: 1.4370 - acc: 0.63 - ETA: 15s - loss: 1.4339 - acc: 0.63 - ETA: 15s - loss: 1.4277 - acc: 0.63 - ETA: 15s - loss: 1.4216 - acc: 0.64 - ETA: 15s - loss: 1.4158 - acc: 0.64 - ETA: 15s - loss: 1.4104 - acc: 0.64 - ETA: 15s - loss: 1.4059 - acc: 0.64 - ETA: 14s - loss: 1.4005 - acc: 0.64 - ETA: 14s - loss: 1.3958 - acc: 0.64 - ETA: 14s - loss: 1.3889 - acc: 0.64 - ETA: 14s - loss: 1.3833 - acc: 0.64 - ETA: 14s - loss: 1.3790 - acc: 0.64 - ETA: 14s - loss: 1.3739 - acc: 0.65 - ETA: 14s - loss: 1.3672 - acc: 0.65 - ETA: 14s - loss: 1.3625 - acc: 0.65 - ETA: 14s - loss: 1.3569 - acc: 0.65 - ETA: 13s - loss: 1.3505 - acc: 0.65 - ETA: 13s - loss: 1.3454 - acc: 0.65 - ETA: 13s - loss: 1.3428 - acc: 0.65 - ETA: 13s - loss: 1.3388 - acc: 0.65 - ETA: 13s - loss: 1.3346 - acc: 0.65 - ETA: 13s - loss: 1.3307 - acc: 0.66 - ETA: 13s - loss: 1.3257 - acc: 0.66 - ETA: 13s - loss: 1.3224 - acc: 0.66 - ETA: 13s - loss: 1.3190 - acc: 0.66 - ETA: 12s - loss: 1.3139 - acc: 0.66 - ETA: 12s - loss: 1.3104 - acc: 0.66 - ETA: 12s - loss: 1.3074 - acc: 0.66 - ETA: 12s - loss: 1.3049 - acc: 0.66 - ETA: 12s - loss: 1.3017 - acc: 0.66 - ETA: 12s - loss: 1.2962 - acc: 0.66 - ETA: 12s - loss: 1.2932 - acc: 0.66 - ETA: 12s - loss: 1.2899 - acc: 0.66 - ETA: 12s - loss: 1.2851 - acc: 0.66 - ETA: 12s - loss: 1.2814 - acc: 0.67 - ETA: 12s - loss: 1.2783 - acc: 0.67 - ETA: 12s - loss: 1.2744 - acc: 0.67 - ETA: 11s - loss: 1.2705 - acc: 0.67 - ETA: 11s - loss: 1.2659 - acc: 0.67 - ETA: 11s - loss: 1.2640 - acc: 0.67 - ETA: 11s - loss: 1.2617 - acc: 0.67 - ETA: 11s - loss: 1.2593 - acc: 0.67 - ETA: 11s - loss: 1.2570 - acc: 0.67 - ETA: 11s - loss: 1.2555 - acc: 0.67 - ETA: 11s - loss: 1.2521 - acc: 0.67 - ETA: 11s - loss: 1.2498 - acc: 0.67 - ETA: 11s - loss: 1.2478 - acc: 0.67 - ETA: 11s - loss: 1.2445 - acc: 0.67 - ETA: 11s - loss: 1.2415 - acc: 0.67 - ETA: 11s - loss: 1.2387 - acc: 0.68 - ETA: 10s - loss: 1.2381 - acc: 0.67 - ETA: 10s - loss: 1.2359 - acc: 0.68 - ETA: 10s - loss: 1.2332 - acc: 0.68 - ETA: 10s - loss: 1.2303 - acc: 0.68 - ETA: 10s - loss: 1.2278 - acc: 0.68 - ETA: 10s - loss: 1.2246 - acc: 0.68 - ETA: 10s - loss: 1.2210 - acc: 0.68 - ETA: 10s - loss: 1.2182 - acc: 0.68 - ETA: 10s - loss: 1.2151 - acc: 0.68 - ETA: 10s - loss: 1.2128 - acc: 0.68 - ETA: 10s - loss: 1.2098 - acc: 0.68 - ETA: 10s - loss: 1.2087 - acc: 0.68 - ETA: 10s - loss: 1.2048 - acc: 0.68 - ETA: 10s - loss: 1.2036 - acc: 0.68 - ETA: 9s - loss: 1.2010 - acc: 0.6873 - ETA: 9s - loss: 1.1995 - acc: 0.687 - ETA: 9s - loss: 1.1957 - acc: 0.688 - ETA: 9s - loss: 1.1942 - acc: 0.688 - ETA: 9s - loss: 1.1936 - acc: 0.689 - ETA: 9s - loss: 1.1928 - acc: 0.689 - ETA: 9s - loss: 1.1905 - acc: 0.689 - ETA: 9s - loss: 1.1883 - acc: 0.689 - ETA: 9s - loss: 1.1834 - acc: 0.690 - ETA: 9s - loss: 1.1793 - acc: 0.691 - ETA: 9s - loss: 1.1778 - acc: 0.692 - ETA: 9s - loss: 1.1762 - acc: 0.692 - ETA: 9s - loss: 1.1742 - acc: 0.692 - ETA: 9s - loss: 1.1724 - acc: 0.692 - ETA: 9s - loss: 1.1710 - acc: 0.693 - ETA: 9s - loss: 1.1682 - acc: 0.693 - ETA: 8s - loss: 1.1665 - acc: 0.693 - ETA: 8s - loss: 1.1648 - acc: 0.694 - ETA: 8s - loss: 1.1619 - acc: 0.694 - ETA: 8s - loss: 1.1602 - acc: 0.695 - ETA: 8s - loss: 1.1578 - acc: 0.696 - ETA: 8s - loss: 1.1559 - acc: 0.696 - ETA: 8s - loss: 1.1541 - acc: 0.696 - ETA: 8s - loss: 1.1524 - acc: 0.697 - ETA: 8s - loss: 1.1518 - acc: 0.697 - ETA: 8s - loss: 1.1492 - acc: 0.697 - ETA: 8s - loss: 1.1472 - acc: 0.698 - ETA: 8s - loss: 1.1453 - acc: 0.698 - ETA: 8s - loss: 1.1441 - acc: 0.698 - ETA: 8s - loss: 1.1434 - acc: 0.698 - ETA: 8s - loss: 1.1420 - acc: 0.699 - ETA: 8s - loss: 1.1396 - acc: 0.699 - ETA: 7s - loss: 1.1375 - acc: 0.699 - ETA: 7s - loss: 1.1364 - acc: 0.699 - ETA: 7s - loss: 1.1353 - acc: 0.700 - ETA: 7s - loss: 1.1328 - acc: 0.700 - ETA: 7s - loss: 1.1308 - acc: 0.701 - ETA: 7s - loss: 1.1296 - acc: 0.701 - ETA: 7s - loss: 1.1293 - acc: 0.701 - ETA: 7s - loss: 1.1271 - acc: 0.701 - ETA: 7s - loss: 1.1255 - acc: 0.702 - ETA: 7s - loss: 1.1239 - acc: 0.702 - ETA: 7s - loss: 1.1224 - acc: 0.702 - ETA: 7s - loss: 1.1204 - acc: 0.702 - ETA: 7s - loss: 1.1179 - acc: 0.7034\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25200/25200 [==============================] - ETA: 7s - loss: 1.1168 - acc: 0.703 - ETA: 7s - loss: 1.1142 - acc: 0.704 - ETA: 6s - loss: 1.1120 - acc: 0.704 - ETA: 6s - loss: 1.1092 - acc: 0.705 - ETA: 6s - loss: 1.1068 - acc: 0.706 - ETA: 6s - loss: 1.1033 - acc: 0.706 - ETA: 6s - loss: 1.1015 - acc: 0.706 - ETA: 6s - loss: 1.0995 - acc: 0.707 - ETA: 6s - loss: 1.0983 - acc: 0.707 - ETA: 6s - loss: 1.0972 - acc: 0.707 - ETA: 6s - loss: 1.0963 - acc: 0.707 - ETA: 6s - loss: 1.0939 - acc: 0.708 - ETA: 6s - loss: 1.0923 - acc: 0.708 - ETA: 6s - loss: 1.0910 - acc: 0.709 - ETA: 6s - loss: 1.0886 - acc: 0.709 - ETA: 6s - loss: 1.0882 - acc: 0.709 - ETA: 5s - loss: 1.0872 - acc: 0.709 - ETA: 5s - loss: 1.0857 - acc: 0.710 - ETA: 5s - loss: 1.0843 - acc: 0.710 - ETA: 5s - loss: 1.0837 - acc: 0.710 - ETA: 5s - loss: 1.0823 - acc: 0.710 - ETA: 5s - loss: 1.0811 - acc: 0.710 - ETA: 5s - loss: 1.0802 - acc: 0.710 - ETA: 5s - loss: 1.0790 - acc: 0.711 - ETA: 5s - loss: 1.0781 - acc: 0.711 - ETA: 5s - loss: 1.0768 - acc: 0.711 - ETA: 5s - loss: 1.0758 - acc: 0.711 - ETA: 5s - loss: 1.0745 - acc: 0.711 - ETA: 5s - loss: 1.0733 - acc: 0.712 - ETA: 5s - loss: 1.0719 - acc: 0.712 - ETA: 5s - loss: 1.0703 - acc: 0.712 - ETA: 5s - loss: 1.0693 - acc: 0.713 - ETA: 5s - loss: 1.0676 - acc: 0.713 - ETA: 5s - loss: 1.0663 - acc: 0.713 - ETA: 4s - loss: 1.0654 - acc: 0.713 - ETA: 4s - loss: 1.0636 - acc: 0.714 - ETA: 4s - loss: 1.0631 - acc: 0.714 - ETA: 4s - loss: 1.0621 - acc: 0.714 - ETA: 4s - loss: 1.0613 - acc: 0.714 - ETA: 4s - loss: 1.0606 - acc: 0.714 - ETA: 4s - loss: 1.0594 - acc: 0.714 - ETA: 4s - loss: 1.0589 - acc: 0.714 - ETA: 4s - loss: 1.0576 - acc: 0.714 - ETA: 4s - loss: 1.0567 - acc: 0.715 - ETA: 4s - loss: 1.0552 - acc: 0.715 - ETA: 4s - loss: 1.0536 - acc: 0.715 - ETA: 4s - loss: 1.0526 - acc: 0.715 - ETA: 4s - loss: 1.0513 - acc: 0.716 - ETA: 4s - loss: 1.0499 - acc: 0.716 - ETA: 4s - loss: 1.0485 - acc: 0.716 - ETA: 4s - loss: 1.0479 - acc: 0.716 - ETA: 4s - loss: 1.0465 - acc: 0.717 - ETA: 3s - loss: 1.0449 - acc: 0.717 - ETA: 3s - loss: 1.0447 - acc: 0.717 - ETA: 3s - loss: 1.0436 - acc: 0.717 - ETA: 3s - loss: 1.0431 - acc: 0.717 - ETA: 3s - loss: 1.0436 - acc: 0.717 - ETA: 3s - loss: 1.0429 - acc: 0.717 - ETA: 3s - loss: 1.0412 - acc: 0.718 - ETA: 3s - loss: 1.0406 - acc: 0.718 - ETA: 3s - loss: 1.0389 - acc: 0.718 - ETA: 3s - loss: 1.0374 - acc: 0.719 - ETA: 3s - loss: 1.0362 - acc: 0.719 - ETA: 3s - loss: 1.0354 - acc: 0.719 - ETA: 3s - loss: 1.0340 - acc: 0.719 - ETA: 3s - loss: 1.0329 - acc: 0.719 - ETA: 3s - loss: 1.0321 - acc: 0.719 - ETA: 3s - loss: 1.0308 - acc: 0.720 - ETA: 3s - loss: 1.0294 - acc: 0.720 - ETA: 3s - loss: 1.0281 - acc: 0.720 - ETA: 2s - loss: 1.0270 - acc: 0.720 - ETA: 2s - loss: 1.0261 - acc: 0.720 - ETA: 2s - loss: 1.0244 - acc: 0.721 - ETA: 2s - loss: 1.0232 - acc: 0.721 - ETA: 2s - loss: 1.0221 - acc: 0.721 - ETA: 2s - loss: 1.0212 - acc: 0.721 - ETA: 2s - loss: 1.0203 - acc: 0.721 - ETA: 2s - loss: 1.0190 - acc: 0.722 - ETA: 2s - loss: 1.0175 - acc: 0.722 - ETA: 2s - loss: 1.0170 - acc: 0.722 - ETA: 2s - loss: 1.0166 - acc: 0.722 - ETA: 2s - loss: 1.0156 - acc: 0.722 - ETA: 2s - loss: 1.0144 - acc: 0.723 - ETA: 2s - loss: 1.0139 - acc: 0.723 - ETA: 2s - loss: 1.0135 - acc: 0.723 - ETA: 2s - loss: 1.0126 - acc: 0.723 - ETA: 2s - loss: 1.0120 - acc: 0.723 - ETA: 1s - loss: 1.0116 - acc: 0.723 - ETA: 1s - loss: 1.0109 - acc: 0.723 - ETA: 1s - loss: 1.0101 - acc: 0.723 - ETA: 1s - loss: 1.0088 - acc: 0.724 - ETA: 1s - loss: 1.0080 - acc: 0.724 - ETA: 1s - loss: 1.0072 - acc: 0.724 - ETA: 1s - loss: 1.0064 - acc: 0.724 - ETA: 1s - loss: 1.0056 - acc: 0.724 - ETA: 1s - loss: 1.0045 - acc: 0.724 - ETA: 1s - loss: 1.0038 - acc: 0.725 - ETA: 1s - loss: 1.0034 - acc: 0.725 - ETA: 1s - loss: 1.0022 - acc: 0.725 - ETA: 1s - loss: 1.0019 - acc: 0.725 - ETA: 1s - loss: 1.0014 - acc: 0.725 - ETA: 1s - loss: 1.0002 - acc: 0.725 - ETA: 1s - loss: 0.9991 - acc: 0.726 - ETA: 1s - loss: 0.9977 - acc: 0.726 - ETA: 1s - loss: 0.9980 - acc: 0.726 - ETA: 0s - loss: 0.9973 - acc: 0.726 - ETA: 0s - loss: 0.9965 - acc: 0.726 - ETA: 0s - loss: 0.9951 - acc: 0.727 - ETA: 0s - loss: 0.9940 - acc: 0.727 - ETA: 0s - loss: 0.9933 - acc: 0.727 - ETA: 0s - loss: 0.9926 - acc: 0.727 - ETA: 0s - loss: 0.9918 - acc: 0.727 - ETA: 0s - loss: 0.9911 - acc: 0.727 - ETA: 0s - loss: 0.9907 - acc: 0.727 - ETA: 0s - loss: 0.9895 - acc: 0.727 - ETA: 0s - loss: 0.9889 - acc: 0.727 - ETA: 0s - loss: 0.9885 - acc: 0.727 - ETA: 0s - loss: 0.9889 - acc: 0.727 - ETA: 0s - loss: 0.9878 - acc: 0.728 - ETA: 0s - loss: 0.9869 - acc: 0.728 - ETA: 0s - loss: 0.9862 - acc: 0.728 - ETA: 0s - loss: 0.9856 - acc: 0.728 - 23s 911us/step - loss: 0.9853 - acc: 0.7282 - val_loss: 0.6294 - val_acc: 0.8064\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16448/25200 [==================>...........] - ETA: 18s - loss: 0.5404 - acc: 0.81 - ETA: 18s - loss: 0.5129 - acc: 0.84 - ETA: 18s - loss: 0.5925 - acc: 0.80 - ETA: 19s - loss: 0.5371 - acc: 0.82 - ETA: 20s - loss: 0.5471 - acc: 0.80 - ETA: 20s - loss: 0.5515 - acc: 0.80 - ETA: 20s - loss: 0.5495 - acc: 0.81 - ETA: 20s - loss: 0.5501 - acc: 0.81 - ETA: 20s - loss: 0.5418 - acc: 0.81 - ETA: 19s - loss: 0.5385 - acc: 0.81 - ETA: 19s - loss: 0.5188 - acc: 0.82 - ETA: 19s - loss: 0.5158 - acc: 0.83 - ETA: 19s - loss: 0.5054 - acc: 0.83 - ETA: 18s - loss: 0.5021 - acc: 0.83 - ETA: 19s - loss: 0.5036 - acc: 0.83 - ETA: 19s - loss: 0.5045 - acc: 0.83 - ETA: 19s - loss: 0.5100 - acc: 0.83 - ETA: 19s - loss: 0.5177 - acc: 0.83 - ETA: 20s - loss: 0.5314 - acc: 0.82 - ETA: 20s - loss: 0.5314 - acc: 0.82 - ETA: 20s - loss: 0.5381 - acc: 0.82 - ETA: 19s - loss: 0.5384 - acc: 0.82 - ETA: 19s - loss: 0.5324 - acc: 0.82 - ETA: 19s - loss: 0.5305 - acc: 0.82 - ETA: 19s - loss: 0.5212 - acc: 0.83 - ETA: 19s - loss: 0.5189 - acc: 0.83 - ETA: 19s - loss: 0.5160 - acc: 0.83 - ETA: 19s - loss: 0.5169 - acc: 0.83 - ETA: 19s - loss: 0.5174 - acc: 0.83 - ETA: 18s - loss: 0.5249 - acc: 0.83 - ETA: 18s - loss: 0.5302 - acc: 0.83 - ETA: 18s - loss: 0.5295 - acc: 0.83 - ETA: 18s - loss: 0.5284 - acc: 0.83 - ETA: 18s - loss: 0.5313 - acc: 0.82 - ETA: 18s - loss: 0.5336 - acc: 0.82 - ETA: 18s - loss: 0.5345 - acc: 0.82 - ETA: 18s - loss: 0.5313 - acc: 0.82 - ETA: 18s - loss: 0.5343 - acc: 0.82 - ETA: 18s - loss: 0.5363 - acc: 0.82 - ETA: 18s - loss: 0.5375 - acc: 0.82 - ETA: 17s - loss: 0.5411 - acc: 0.82 - ETA: 17s - loss: 0.5401 - acc: 0.82 - ETA: 17s - loss: 0.5426 - acc: 0.82 - ETA: 17s - loss: 0.5379 - acc: 0.82 - ETA: 17s - loss: 0.5352 - acc: 0.83 - ETA: 17s - loss: 0.5342 - acc: 0.83 - ETA: 17s - loss: 0.5329 - acc: 0.83 - ETA: 17s - loss: 0.5336 - acc: 0.83 - ETA: 17s - loss: 0.5315 - acc: 0.83 - ETA: 17s - loss: 0.5311 - acc: 0.83 - ETA: 17s - loss: 0.5272 - acc: 0.83 - ETA: 17s - loss: 0.5282 - acc: 0.83 - ETA: 16s - loss: 0.5276 - acc: 0.83 - ETA: 16s - loss: 0.5276 - acc: 0.83 - ETA: 16s - loss: 0.5281 - acc: 0.83 - ETA: 16s - loss: 0.5271 - acc: 0.83 - ETA: 16s - loss: 0.5257 - acc: 0.83 - ETA: 16s - loss: 0.5257 - acc: 0.83 - ETA: 16s - loss: 0.5248 - acc: 0.83 - ETA: 16s - loss: 0.5222 - acc: 0.83 - ETA: 16s - loss: 0.5227 - acc: 0.83 - ETA: 16s - loss: 0.5242 - acc: 0.83 - ETA: 16s - loss: 0.5253 - acc: 0.83 - ETA: 16s - loss: 0.5234 - acc: 0.83 - ETA: 16s - loss: 0.5234 - acc: 0.83 - ETA: 16s - loss: 0.5229 - acc: 0.83 - ETA: 15s - loss: 0.5197 - acc: 0.83 - ETA: 15s - loss: 0.5192 - acc: 0.83 - ETA: 15s - loss: 0.5187 - acc: 0.83 - ETA: 15s - loss: 0.5191 - acc: 0.83 - ETA: 15s - loss: 0.5212 - acc: 0.83 - ETA: 15s - loss: 0.5217 - acc: 0.83 - ETA: 15s - loss: 0.5217 - acc: 0.83 - ETA: 15s - loss: 0.5207 - acc: 0.83 - ETA: 15s - loss: 0.5193 - acc: 0.83 - ETA: 15s - loss: 0.5209 - acc: 0.83 - ETA: 15s - loss: 0.5207 - acc: 0.83 - ETA: 15s - loss: 0.5201 - acc: 0.83 - ETA: 15s - loss: 0.5200 - acc: 0.83 - ETA: 15s - loss: 0.5200 - acc: 0.83 - ETA: 15s - loss: 0.5212 - acc: 0.83 - ETA: 15s - loss: 0.5199 - acc: 0.83 - ETA: 14s - loss: 0.5224 - acc: 0.83 - ETA: 14s - loss: 0.5257 - acc: 0.83 - ETA: 14s - loss: 0.5260 - acc: 0.83 - ETA: 14s - loss: 0.5244 - acc: 0.83 - ETA: 14s - loss: 0.5253 - acc: 0.83 - ETA: 14s - loss: 0.5240 - acc: 0.83 - ETA: 14s - loss: 0.5223 - acc: 0.83 - ETA: 14s - loss: 0.5219 - acc: 0.83 - ETA: 14s - loss: 0.5214 - acc: 0.83 - ETA: 14s - loss: 0.5227 - acc: 0.83 - ETA: 14s - loss: 0.5217 - acc: 0.83 - ETA: 14s - loss: 0.5212 - acc: 0.83 - ETA: 14s - loss: 0.5197 - acc: 0.83 - ETA: 14s - loss: 0.5195 - acc: 0.83 - ETA: 14s - loss: 0.5190 - acc: 0.83 - ETA: 13s - loss: 0.5178 - acc: 0.83 - ETA: 13s - loss: 0.5186 - acc: 0.83 - ETA: 13s - loss: 0.5178 - acc: 0.83 - ETA: 13s - loss: 0.5185 - acc: 0.83 - ETA: 13s - loss: 0.5190 - acc: 0.83 - ETA: 13s - loss: 0.5188 - acc: 0.83 - ETA: 13s - loss: 0.5189 - acc: 0.83 - ETA: 13s - loss: 0.5213 - acc: 0.83 - ETA: 13s - loss: 0.5223 - acc: 0.83 - ETA: 13s - loss: 0.5218 - acc: 0.83 - ETA: 13s - loss: 0.5212 - acc: 0.83 - ETA: 13s - loss: 0.5206 - acc: 0.83 - ETA: 13s - loss: 0.5191 - acc: 0.83 - ETA: 13s - loss: 0.5192 - acc: 0.83 - ETA: 13s - loss: 0.5176 - acc: 0.83 - ETA: 13s - loss: 0.5168 - acc: 0.83 - ETA: 13s - loss: 0.5167 - acc: 0.83 - ETA: 12s - loss: 0.5161 - acc: 0.83 - ETA: 12s - loss: 0.5147 - acc: 0.83 - ETA: 12s - loss: 0.5156 - acc: 0.83 - ETA: 12s - loss: 0.5148 - acc: 0.83 - ETA: 12s - loss: 0.5157 - acc: 0.83 - ETA: 12s - loss: 0.5173 - acc: 0.83 - ETA: 12s - loss: 0.5176 - acc: 0.83 - ETA: 12s - loss: 0.5184 - acc: 0.83 - ETA: 12s - loss: 0.5170 - acc: 0.83 - ETA: 12s - loss: 0.5174 - acc: 0.83 - ETA: 12s - loss: 0.5185 - acc: 0.83 - ETA: 12s - loss: 0.5177 - acc: 0.83 - ETA: 12s - loss: 0.5166 - acc: 0.83 - ETA: 12s - loss: 0.5173 - acc: 0.83 - ETA: 12s - loss: 0.5174 - acc: 0.83 - ETA: 12s - loss: 0.5184 - acc: 0.83 - ETA: 12s - loss: 0.5183 - acc: 0.83 - ETA: 12s - loss: 0.5189 - acc: 0.83 - ETA: 11s - loss: 0.5189 - acc: 0.83 - ETA: 11s - loss: 0.5201 - acc: 0.83 - ETA: 11s - loss: 0.5198 - acc: 0.83 - ETA: 11s - loss: 0.5189 - acc: 0.83 - ETA: 11s - loss: 0.5196 - acc: 0.83 - ETA: 11s - loss: 0.5217 - acc: 0.83 - ETA: 11s - loss: 0.5206 - acc: 0.83 - ETA: 11s - loss: 0.5213 - acc: 0.83 - ETA: 11s - loss: 0.5216 - acc: 0.83 - ETA: 11s - loss: 0.5219 - acc: 0.83 - ETA: 11s - loss: 0.5227 - acc: 0.83 - ETA: 11s - loss: 0.5224 - acc: 0.83 - ETA: 11s - loss: 0.5217 - acc: 0.83 - ETA: 11s - loss: 0.5221 - acc: 0.83 - ETA: 11s - loss: 0.5231 - acc: 0.83 - ETA: 10s - loss: 0.5229 - acc: 0.83 - ETA: 10s - loss: 0.5229 - acc: 0.83 - ETA: 10s - loss: 0.5240 - acc: 0.83 - ETA: 10s - loss: 0.5237 - acc: 0.83 - ETA: 10s - loss: 0.5237 - acc: 0.83 - ETA: 10s - loss: 0.5239 - acc: 0.83 - ETA: 10s - loss: 0.5240 - acc: 0.83 - ETA: 10s - loss: 0.5248 - acc: 0.83 - ETA: 10s - loss: 0.5246 - acc: 0.83 - ETA: 10s - loss: 0.5243 - acc: 0.83 - ETA: 10s - loss: 0.5244 - acc: 0.83 - ETA: 10s - loss: 0.5236 - acc: 0.83 - ETA: 10s - loss: 0.5246 - acc: 0.83 - ETA: 10s - loss: 0.5244 - acc: 0.83 - ETA: 10s - loss: 0.5243 - acc: 0.83 - ETA: 9s - loss: 0.5250 - acc: 0.8320 - ETA: 9s - loss: 0.5246 - acc: 0.832 - ETA: 9s - loss: 0.5241 - acc: 0.832 - ETA: 9s - loss: 0.5241 - acc: 0.832 - ETA: 9s - loss: 0.5240 - acc: 0.832 - ETA: 9s - loss: 0.5242 - acc: 0.832 - ETA: 9s - loss: 0.5250 - acc: 0.832 - ETA: 9s - loss: 0.5250 - acc: 0.832 - ETA: 9s - loss: 0.5243 - acc: 0.832 - ETA: 9s - loss: 0.5240 - acc: 0.832 - ETA: 9s - loss: 0.5242 - acc: 0.832 - ETA: 9s - loss: 0.5240 - acc: 0.832 - ETA: 9s - loss: 0.5252 - acc: 0.832 - ETA: 9s - loss: 0.5272 - acc: 0.831 - ETA: 9s - loss: 0.5269 - acc: 0.832 - ETA: 8s - loss: 0.5263 - acc: 0.832 - ETA: 8s - loss: 0.5270 - acc: 0.832 - ETA: 8s - loss: 0.5268 - acc: 0.832 - ETA: 8s - loss: 0.5269 - acc: 0.832 - ETA: 8s - loss: 0.5263 - acc: 0.832 - ETA: 8s - loss: 0.5260 - acc: 0.832 - ETA: 8s - loss: 0.5269 - acc: 0.832 - ETA: 8s - loss: 0.5267 - acc: 0.832 - ETA: 8s - loss: 0.5265 - acc: 0.832 - ETA: 8s - loss: 0.5271 - acc: 0.831 - ETA: 8s - loss: 0.5274 - acc: 0.831 - ETA: 8s - loss: 0.5280 - acc: 0.831 - ETA: 8s - loss: 0.5279 - acc: 0.831 - ETA: 8s - loss: 0.5282 - acc: 0.831 - ETA: 8s - loss: 0.5287 - acc: 0.831 - ETA: 7s - loss: 0.5283 - acc: 0.831 - ETA: 7s - loss: 0.5287 - acc: 0.830 - ETA: 7s - loss: 0.5277 - acc: 0.831 - ETA: 7s - loss: 0.5282 - acc: 0.831 - ETA: 7s - loss: 0.5275 - acc: 0.831 - ETA: 7s - loss: 0.5271 - acc: 0.831 - ETA: 7s - loss: 0.5275 - acc: 0.831 - ETA: 7s - loss: 0.5278 - acc: 0.831 - ETA: 7s - loss: 0.5284 - acc: 0.830 - ETA: 7s - loss: 0.5289 - acc: 0.830 - ETA: 7s - loss: 0.5291 - acc: 0.830 - ETA: 7s - loss: 0.5294 - acc: 0.830 - ETA: 7s - loss: 0.5299 - acc: 0.830 - ETA: 7s - loss: 0.5308 - acc: 0.830 - ETA: 7s - loss: 0.5317 - acc: 0.829 - ETA: 7s - loss: 0.5312 - acc: 0.830 - ETA: 7s - loss: 0.5314 - acc: 0.829 - ETA: 7s - loss: 0.5319 - acc: 0.829 - ETA: 7s - loss: 0.5322 - acc: 0.829 - ETA: 7s - loss: 0.5323 - acc: 0.829 - ETA: 7s - loss: 0.5329 - acc: 0.829 - ETA: 6s - loss: 0.5319 - acc: 0.830 - ETA: 6s - loss: 0.5317 - acc: 0.8303"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25200/25200 [==============================] - ETA: 6s - loss: 0.5332 - acc: 0.830 - ETA: 6s - loss: 0.5330 - acc: 0.829 - ETA: 6s - loss: 0.5325 - acc: 0.830 - ETA: 6s - loss: 0.5322 - acc: 0.830 - ETA: 6s - loss: 0.5325 - acc: 0.829 - ETA: 6s - loss: 0.5323 - acc: 0.830 - ETA: 6s - loss: 0.5321 - acc: 0.829 - ETA: 6s - loss: 0.5325 - acc: 0.829 - ETA: 6s - loss: 0.5344 - acc: 0.829 - ETA: 6s - loss: 0.5342 - acc: 0.829 - ETA: 6s - loss: 0.5337 - acc: 0.829 - ETA: 6s - loss: 0.5343 - acc: 0.829 - ETA: 6s - loss: 0.5347 - acc: 0.829 - ETA: 6s - loss: 0.5352 - acc: 0.829 - ETA: 6s - loss: 0.5350 - acc: 0.829 - ETA: 5s - loss: 0.5351 - acc: 0.829 - ETA: 5s - loss: 0.5353 - acc: 0.829 - ETA: 5s - loss: 0.5350 - acc: 0.829 - ETA: 5s - loss: 0.5346 - acc: 0.829 - ETA: 5s - loss: 0.5339 - acc: 0.829 - ETA: 5s - loss: 0.5342 - acc: 0.829 - ETA: 5s - loss: 0.5343 - acc: 0.829 - ETA: 5s - loss: 0.5338 - acc: 0.829 - ETA: 5s - loss: 0.5340 - acc: 0.829 - ETA: 5s - loss: 0.5337 - acc: 0.829 - ETA: 5s - loss: 0.5330 - acc: 0.829 - ETA: 5s - loss: 0.5333 - acc: 0.829 - ETA: 5s - loss: 0.5333 - acc: 0.829 - ETA: 5s - loss: 0.5333 - acc: 0.829 - ETA: 5s - loss: 0.5329 - acc: 0.829 - ETA: 5s - loss: 0.5327 - acc: 0.829 - ETA: 5s - loss: 0.5324 - acc: 0.829 - ETA: 5s - loss: 0.5327 - acc: 0.829 - ETA: 5s - loss: 0.5326 - acc: 0.829 - ETA: 4s - loss: 0.5326 - acc: 0.829 - ETA: 4s - loss: 0.5331 - acc: 0.828 - ETA: 4s - loss: 0.5329 - acc: 0.829 - ETA: 4s - loss: 0.5330 - acc: 0.829 - ETA: 4s - loss: 0.5324 - acc: 0.829 - ETA: 4s - loss: 0.5319 - acc: 0.829 - ETA: 4s - loss: 0.5326 - acc: 0.829 - ETA: 4s - loss: 0.5328 - acc: 0.829 - ETA: 4s - loss: 0.5324 - acc: 0.829 - ETA: 4s - loss: 0.5342 - acc: 0.828 - ETA: 4s - loss: 0.5346 - acc: 0.828 - ETA: 4s - loss: 0.5351 - acc: 0.828 - ETA: 4s - loss: 0.5351 - acc: 0.828 - ETA: 4s - loss: 0.5347 - acc: 0.828 - ETA: 4s - loss: 0.5348 - acc: 0.828 - ETA: 4s - loss: 0.5341 - acc: 0.828 - ETA: 4s - loss: 0.5343 - acc: 0.828 - ETA: 3s - loss: 0.5338 - acc: 0.829 - ETA: 3s - loss: 0.5333 - acc: 0.829 - ETA: 3s - loss: 0.5331 - acc: 0.829 - ETA: 3s - loss: 0.5333 - acc: 0.829 - ETA: 3s - loss: 0.5331 - acc: 0.829 - ETA: 3s - loss: 0.5335 - acc: 0.829 - ETA: 3s - loss: 0.5335 - acc: 0.829 - ETA: 3s - loss: 0.5333 - acc: 0.829 - ETA: 3s - loss: 0.5339 - acc: 0.829 - ETA: 3s - loss: 0.5336 - acc: 0.829 - ETA: 3s - loss: 0.5333 - acc: 0.829 - ETA: 3s - loss: 0.5331 - acc: 0.829 - ETA: 3s - loss: 0.5328 - acc: 0.829 - ETA: 3s - loss: 0.5332 - acc: 0.829 - ETA: 3s - loss: 0.5339 - acc: 0.828 - ETA: 3s - loss: 0.5345 - acc: 0.828 - ETA: 3s - loss: 0.5345 - acc: 0.828 - ETA: 2s - loss: 0.5339 - acc: 0.829 - ETA: 2s - loss: 0.5336 - acc: 0.829 - ETA: 2s - loss: 0.5343 - acc: 0.828 - ETA: 2s - loss: 0.5344 - acc: 0.828 - ETA: 2s - loss: 0.5341 - acc: 0.828 - ETA: 2s - loss: 0.5338 - acc: 0.829 - ETA: 2s - loss: 0.5352 - acc: 0.828 - ETA: 2s - loss: 0.5349 - acc: 0.828 - ETA: 2s - loss: 0.5353 - acc: 0.828 - ETA: 2s - loss: 0.5353 - acc: 0.828 - ETA: 2s - loss: 0.5354 - acc: 0.828 - ETA: 2s - loss: 0.5351 - acc: 0.828 - ETA: 2s - loss: 0.5350 - acc: 0.828 - ETA: 2s - loss: 0.5348 - acc: 0.828 - ETA: 2s - loss: 0.5352 - acc: 0.828 - ETA: 2s - loss: 0.5350 - acc: 0.828 - ETA: 2s - loss: 0.5348 - acc: 0.828 - ETA: 2s - loss: 0.5347 - acc: 0.828 - ETA: 2s - loss: 0.5351 - acc: 0.828 - ETA: 1s - loss: 0.5350 - acc: 0.828 - ETA: 1s - loss: 0.5342 - acc: 0.828 - ETA: 1s - loss: 0.5344 - acc: 0.828 - ETA: 1s - loss: 0.5347 - acc: 0.828 - ETA: 1s - loss: 0.5343 - acc: 0.828 - ETA: 1s - loss: 0.5340 - acc: 0.828 - ETA: 1s - loss: 0.5350 - acc: 0.828 - ETA: 1s - loss: 0.5351 - acc: 0.828 - ETA: 1s - loss: 0.5350 - acc: 0.828 - ETA: 1s - loss: 0.5354 - acc: 0.828 - ETA: 1s - loss: 0.5358 - acc: 0.828 - ETA: 1s - loss: 0.5369 - acc: 0.827 - ETA: 1s - loss: 0.5377 - acc: 0.827 - ETA: 1s - loss: 0.5380 - acc: 0.827 - ETA: 1s - loss: 0.5385 - acc: 0.827 - ETA: 1s - loss: 0.5390 - acc: 0.826 - ETA: 1s - loss: 0.5394 - acc: 0.826 - ETA: 1s - loss: 0.5395 - acc: 0.826 - ETA: 1s - loss: 0.5398 - acc: 0.826 - ETA: 0s - loss: 0.5396 - acc: 0.826 - ETA: 0s - loss: 0.5394 - acc: 0.826 - ETA: 0s - loss: 0.5390 - acc: 0.826 - ETA: 0s - loss: 0.5387 - acc: 0.827 - ETA: 0s - loss: 0.5385 - acc: 0.827 - ETA: 0s - loss: 0.5383 - acc: 0.827 - ETA: 0s - loss: 0.5383 - acc: 0.827 - ETA: 0s - loss: 0.5383 - acc: 0.827 - ETA: 0s - loss: 0.5381 - acc: 0.827 - ETA: 0s - loss: 0.5383 - acc: 0.827 - ETA: 0s - loss: 0.5380 - acc: 0.827 - ETA: 0s - loss: 0.5382 - acc: 0.827 - ETA: 0s - loss: 0.5389 - acc: 0.826 - ETA: 0s - loss: 0.5395 - acc: 0.826 - ETA: 0s - loss: 0.5394 - acc: 0.826 - ETA: 0s - loss: 0.5396 - acc: 0.826 - ETA: 0s - loss: 0.5396 - acc: 0.826 - ETA: 0s - loss: 0.5400 - acc: 0.826 - ETA: 0s - loss: 0.5404 - acc: 0.826 - 21s 819us/step - loss: 0.5406 - acc: 0.8263 - val_loss: 0.6176 - val_acc: 0.8132\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 2s 180us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5974958278735478, 0.8086666666666666]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Class Text Classification Model Comparison and Selection  \n",
    "https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
