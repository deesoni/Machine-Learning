{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_docs = [\"Here are some very simple basic sentences.\",\n",
    "\"They won't be very interesting, I'm afraid.\",\n",
    "\"The point of these examples is to _learn how basic text cleaning works_ on *very simple* data.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences', '.'],\n",
       " ['They',\n",
       "  'wo',\n",
       "  \"n't\",\n",
       "  'be',\n",
       "  'very',\n",
       "  'interesting',\n",
       "  ',',\n",
       "  'I',\n",
       "  \"'m\",\n",
       "  'afraid',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'point',\n",
       "  'of',\n",
       "  'these',\n",
       "  'examples',\n",
       "  'is',\n",
       "  'to',\n",
       "  '_learn',\n",
       "  'how',\n",
       "  'basic',\n",
       "  'text',\n",
       "  'cleaning',\n",
       "  'works_',\n",
       "  'on',\n",
       "  '*very',\n",
       "  'simple*',\n",
       "  'data',\n",
       "  '.']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_doc = [word_tokenize(doc) for doc in raw_docs]\n",
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences'],\n",
       " ['They', 'wo', 'nt', 'be', 'very', 'interesting', 'I', 'm', 'afraid'],\n",
       " ['The',\n",
       "  'point',\n",
       "  'of',\n",
       "  'these',\n",
       "  'examples',\n",
       "  'is',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'how',\n",
       "  'basic',\n",
       "  'text',\n",
       "  'cleaning',\n",
       "  'works',\n",
       "  'on',\n",
       "  'very',\n",
       "  'simple',\n",
       "  'data']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = re.compile('[%s]' % re.escape(punctuation))\n",
    "\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for doc in tokenized_doc:\n",
    "    new_doc = []\n",
    "    for token in doc:\n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_doc.append(new_token)\n",
    "            \n",
    "    tokenized_docs_no_punctuation.append(new_doc)\n",
    "    \n",
    "tokenized_docs_no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Here', 'simple', 'basic', 'sentences'],\n",
       " ['They', 'wo', 'nt', 'interesting', 'I', 'afraid'],\n",
       " ['The',\n",
       "  'point',\n",
       "  'examples',\n",
       "  'learn',\n",
       "  'basic',\n",
       "  'text',\n",
       "  'cleaning',\n",
       "  'works',\n",
       "  'simple',\n",
       "  'data']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_docs_no_stopwords = []\n",
    "\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    new_doc = [word for word in doc if word not in stopwords.words('english')]\n",
    "    tokenized_docs_no_stopwords.append(new_doc)\n",
    "\n",
    "tokenized_docs_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['here', 'simpl', 'basic', 'sentenc'],\n",
       " ['they', 'wo', 'nt', 'interest', 'I', 'afraid'],\n",
       " ['the',\n",
       "  'point',\n",
       "  'exampl',\n",
       "  'learn',\n",
       "  'basic',\n",
       "  'text',\n",
       "  'clean',\n",
       "  'work',\n",
       "  'simpl',\n",
       "  'data']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "preprocessed_docs = []\n",
    "\n",
    "for doc in tokenized_docs_no_stopwords:\n",
    "    new_doc = [porter.stem(word) for word in doc]\n",
    "    preprocessed_docs.append(new_doc)\n",
    "    \n",
    "preprocessed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
